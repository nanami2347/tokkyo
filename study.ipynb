{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvbSQf5einUc",
        "outputId": "0cdf9944-e625-4e71-ec1d-0a6e8ffc0d8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p \"/content/drive/My Drive/work/\"\n",
        "%cd /content/drive/My Drive/work/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38gJu-dbise0",
        "outputId": "f4fffcbf-d2e5-4afe-c02c-27e6fd6eeac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6KJOAVbixEd",
        "outputId": "c84b0478-7442-4a52-b022-e6532a06fb20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "fkFalOmti1ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "LwMLZDr2i8Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G1roZWpFXPpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('json', data_files=\"/content/drive/My Drive/tokkyo\", split=\"train\")\n",
        "\n",
        "def generate_prompt(data_point):\n",
        "    result = f\"\"\"[INST] {data_point[\"instruction\"]} [/INST] {data_point[\"output\"]}\"\"\"\n",
        "    return result\n",
        "\n",
        "def add_text(example):\n",
        "    example[\"text\"] = generate_prompt(example)\n",
        "    del example[\"instruction\"]\n",
        "    del example[\"output\"]\n",
        "\n",
        "    return example\n",
        "dataset = dataset.map(add_text)\n",
        "print(dataset[\"text\"])"
      ],
      "metadata": {
        "id": "dnZQsJ3EjAjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ],
      "metadata": {
        "id": "K-zhdU5UjEO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    use_fast=False,\n",
        "    add_eos_token=True,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "2dAyBCv2jHjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"k_proj\", \"v_proj\"]\n",
        ")\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./train_logs\",\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    max_steps=5,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    weight_decay=0.001,\n",
        "    save_steps=25,\n",
        "    logging_steps=25,\n",
        "    group_by_length=True,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    peft_config=peft_config,\n",
        "    args=training_arguments,\n",
        "    max_seq_length=None,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.model.save_pretrained(\"./lora_model\")"
      ],
      "metadata": {
        "id": "YSTIbygUjNb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    \"./lora_model\",\n",
        "    torch_dtype=torch.float16,\n",
        "    #load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "_-GKp1pDkh0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.merge_and_unload()\n",
        "model.save_pretrained(\"./marged_model\", safe_serialization=True)"
      ],
      "metadata": {
        "id": "rRBocclzkqIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"./marged_model\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    load_in_4bit=True,\n",
        "    device_map={\"\": 0},\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "NL9Q9jL_kt11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"[INST] 米飯に用いたとき炊飯斑や加熱斑が発生しない発明は？ [/INST] \"\n",
        "\n",
        "input_ids = tokenizer(prompt, add_special_tokens=False, return_tensors='pt')\n",
        "output_ids = model.generate(\n",
        "    **input_ids.to(model.device),\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    temperature=0.3,\n",
        ")\n",
        "output = tokenizer.decode(output_ids.tolist()[0])\n",
        "print(output)"
      ],
      "metadata": {
        "id": "EhlLbi9HkxI1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}